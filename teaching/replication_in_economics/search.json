[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Replication in Economics: A Practical Guide",
    "section": "",
    "text": "1 Syllabus\nThis course provides a practical introduction to the essential elements of research replication in economics. It covers project organization, data management, coding practices in Stata and Python, and collaborative version control using Git. The primary goal is to equip students with the skills to conduct replicable research and to navigate the replication process effectively.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Replication in Economics: A Practical Guide</span>"
    ]
  },
  {
    "objectID": "index.html#course-objectives",
    "href": "index.html#course-objectives",
    "title": "Replication in Economics: A Practical Guide",
    "section": "1.1 Course Objectives",
    "text": "1.1 Course Objectives\nUpon completion of this course, students will be able to:\n\nOrganize research projects in a structured and reproducible manner.\nManage and document data effectively.\nWrite clear, well-documented code in both Stata and Python.\nUtilize Git for version control and collaboration.\nUnderstand the importance of replication in economics and the standards expected by journals.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Replication in Economics: A Practical Guide</span>"
    ]
  },
  {
    "objectID": "index.html#course-content",
    "href": "index.html#course-content",
    "title": "Replication in Economics: A Practical Guide",
    "section": "1.2 Course Content",
    "text": "1.2 Course Content\n\nProject Organization:\n\nSetting up project folders and directory structures.\nCreating and maintaining a comprehensive README file.\n\nData Management:\n\nBest practices for handling and documenting data.\nEnsuring data integrity and accessibility.\n\nReplication with Stata:\n\nCoding standards and practices for replicable Stata code.\nCommon pitfalls and how to avoid them.\n\nReplication with Python and Quarto:\n\nLeveraging Python and Quarto for reproducible research workflows.\nIntegrating code, data, and documentation.\n\nVersion Control with Git:\n\nUsing Git for tracking changes and collaborating with others.\nManaging branches, merging, and resolving conflicts.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Replication in Economics: A Practical Guide</span>"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "2  Introduction",
    "section": "",
    "text": "2.1 Purpose and Reason d’être\nReplication has become an increasingly important topic in economics. As the field has grown more empirical and data-driven, the ability to reproduce published results is essential for scientific credibility. The so-called “replication crisis” –highlighted humorously in this XKCD comic– has affected not only economics but many scientific disciplines.\nThe main purpose of replication is to verify that published results can be independently reproduced using the same data and code. This process helps ensure that findings are not the result of errors, selective reporting, or other issues. Replication is a fundamental part of the scientific method, fostering transparency, accountability, and trust in research.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#purpose-and-reason-dêtre",
    "href": "introduction.html#purpose-and-reason-dêtre",
    "title": "2  Introduction",
    "section": "",
    "text": "2.1.1 Types of Replication\nThere are several types of replication in economics, each serving a different purpose (Clemens 2017):\n\nPure (or direct) replication: Reproducing the results of a study using the same data and methods as the original paper.\nRobustness replication: Testing whether the results hold under alternative methods, model specifications, or subsets of the data.\nConceptual replication: Examining whether the main findings hold in different contexts, with different data, or using different approaches.\n\nUnderstanding these distinctions is important, as each type of replication addresses different aspects of scientific validity and reliability. This course focuses on the first type, namely, how to organize and document research projects to facilitate pure replication.\n\n\n2.1.2 Limits of Replication\nHowever, replication has its limits:\n\nCode errors: Replication only checks if the code produces the reported results, not whether the code itself is correct. For example, see this case of undocumented code alterations.\nMismatch between code and text: Sometimes, the code does not follow the procedures described in the paper. See this study for an example.\nPeer review limitations: Code is often not reviewed by referees, so errors or inconsistencies may go unnoticed.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#the-value-of-sharing-data-and-methods",
    "href": "introduction.html#the-value-of-sharing-data-and-methods",
    "title": "2  Introduction",
    "section": "2.2 The Value of Sharing Data and Methods",
    "text": "2.2 The Value of Sharing Data and Methods\nBy publishing the data and methods used in research, economists facilitate the reuse of data and code. This practice, inspired by the open source movement, enables other researchers to build upon existing work, verify results, and accelerate scientific progress. Open sharing not only increases transparency but also fosters collaboration and innovation, as others can adapt and extend methods for new questions and contexts.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#additional-considerations",
    "href": "introduction.html#additional-considerations",
    "title": "2  Introduction",
    "section": "2.3 Additional Considerations",
    "text": "2.3 Additional Considerations\n\nOpen science movement: Many journals now require authors to share data and code, promoting transparency.\nChallenges: Replication can be hindered by restricted data access, proprietary software, or poor documentation.\n\n\n\n\n\nClemens, Michael A. 2017. “The Meaning of Failed Replications: A Review and Proposal.” Journal of Economic Surveys 31 (1): 326–42.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "preparing_readme_file.html",
    "href": "preparing_readme_file.html",
    "title": "3  Preparing the README",
    "section": "",
    "text": "3.1 The README\nA README is not just a courtesy—it’s a requirement for replication packages in leading journals. It must provide a clear, complete, and transparent record of your project, including precise data provenance and licensing information for every dataset used. This is essential for transparency, reproducibility, and legal compliance.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Preparing the README</span>"
    ]
  },
  {
    "objectID": "preparing_readme_file.html#the-readme",
    "href": "preparing_readme_file.html#the-readme",
    "title": "3  Preparing the README",
    "section": "",
    "text": "3.1.1 What Must Be Included?\nEvery time you download a dataset, you must record:\n\nSource: Where did you get the data? (URL, repository, or contact)\nDownload Date: When did you obtain it?\nLicense/Terms of Use: Under what conditions can it be used or shared?\nAccess Conditions: Is registration required? Is it public, restricted, or proprietary?\nVersion/Checksum: If available, note the version or a checksum to ensure exact replication.\nAny Modifications: If you renamed, reformatted, or processed the data, document exactly what was done and where.\n\nThis information should be included in your README under a Data Availability and Provenance section, as required by journals.\n\n\n3.1.2 Essential README Sections\nA robust README should include:\n\nTitle and Authors: With affiliations and contact info.\nOverview: What does the replication package do? What is the main research question?\nData Availability and Provenance: For each dataset, provide all details above.\nSoftware Requirements: List all software and versions needed.\nInstallation Instructions: How to set up the environment and dependencies.\nRunning the Code: Step-by-step instructions for reproducing results.\nFile Structure: Briefly describe the folder and file organization.\nKnown Issues or Caveats: Any limitations or platform-specific notes.\nCitation: How to cite the original paper and this replication package.\nReferences: Full bibliographic details for all data and code sources.\n\n\n\n\n\n\n\nTip\n\n\n\nUpdate your README as you go. Don’t wait until the end—details are easily forgotten!",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Preparing the README</span>"
    ]
  },
  {
    "objectID": "preparing_readme_file.html#simplified-readme-template",
    "href": "preparing_readme_file.html#simplified-readme-template",
    "title": "3  Preparing the README",
    "section": "3.2 Simplified README Template",
    "text": "3.2 Simplified README Template\nBelow is a concise template you can adapt for your own projects. Use this as a starting point, but always include all required details for each dataset. Some journals also request a correspondence table indicating which files and lines of code correspond to each figure and table in the paper.\n# Replication Package for: [Paper Title]\n\n**Authors:** [Name] ([email]), [Name] ([email])  \n**Affiliations:** [Institution(s)]  \n**Date:** [YYYY-MM-DD]\n\n## Overview\n\nThis package replicates the results in [Authors] ([Year]), \"[Paper Title]\", [Journal Name], [Volume(Issue)], [Pages].  \nIt includes all code and data necessary to reproduce the tables and figures in the paper.\n\n## Data Availability and Provenance\n\n| Dataset | Source/URL | Download Date | License | Access Conditions | Modifications |\n|---------|------------|---------------|---------|------------------|---------------|\n| [Dataset 1] | [URL] | [YYYY-MM-DD] | [e.g. CC BY 4.0] | [e.g. Public/Registration required] | [e.g. Renamed columns, see `code/prepare_data.py`] |\n| [Dataset 2] | [URL] | [YYYY-MM-DD] | [License] | [Access] | [Modifications] |\n| ... | ... | ... | ... | ... | ... |\n\n*All original data files are stored in `data/original/`. Do not modify these files.*\n\n## Software and Hardware Requirements\n\n- [Stata 17+ / Python 3.10+ / R 4.2+ / ...]\n- [List any required packages or libraries]\n- [Any specific hardware requirements, e.g. RAM, CPU]\n\nThe code has been tested on [Operating System(s), e.g. Windows 10, Ubuntu 20.04].\n\n## Running the Code\n\n1. Adjust any file paths as needed (see instructions in `code/`).\n2. Run the main script(s):\n    - For Stata: `do code/main.do`\n    - For Python: `python code/analysis.py`\n    - For LaTeX: Compile `paper/paper.tex`\n\n## File Structure\n\n- `data/`: Raw and processed data\n- `code/`: Scripts for data cleaning and analysis\n- `paper/`: Manuscript and output files\n\n## Correspondence Table\n\n| Figure/Table | File | Line Numbers |\n|--------------|------|--------------|\n|              |      |              |\n\n## Citation\n\n[Authors] ([Year]). \"[Paper Title]\". [Journal Name], [Volume(Issue)], [Pages].  \nReplication package: [URL or DOI]\n\n## References\n\n- [Full citations for all datasets and software used]\nWhen preparing your replication package, make sure that you are allowed to redistribute all data and code included in the package. This is crucial for compliance with copyright and licensing agreements. It is also good practice to test your package by copying only the original files and code into a new directory and running the analysis from there.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Preparing the README</span>"
    ]
  },
  {
    "objectID": "preparing_readme_file.html#example-readme-file",
    "href": "preparing_readme_file.html#example-readme-file",
    "title": "3  Preparing the README",
    "section": "3.3 Example README file",
    "text": "3.3 Example README file\n# Replication Package for: Solar Eclipses and the Origins of Critical Thinking and Complexity.\n## Date 2023-03-06\n\n## Overview\nThis replication package accompanies Litina and Roca-Fernández. (forthcoming).\n\"Solar Eclipses and the Origins of Critical Thinking and Complexity\". The Economic Journal. DOI.\n\nThis articles uses multiple data sources that are described below.\n20 interconnected Stata files run the code to generate the results for the 20 figures and 19 tables in the paper, including those in the Appendices.\nAll the required data are available in open and Stata formats.\nThe replicator should expect the code to run for about 10 minutes.\n\n### Computational requirements\nThe replicator is expected to have an up-to-date version of Stata 17 and Python (version 3.10 or more recent).\nThe code was thoroughly tested on Linux with kernel version 6.3.7.\nStata do-files should run on other operating systems, except for calls to the program `echo` that is only natively available on Linux, MacOS, and BSD systems.\nLacking it does not hinder the replication exercise.\nNote that running the Python code on Windows may require some changes to the scripts.\nThere are no special requirements in terms of hardware.\n\n## Authors\n\n- Anastasia Litina\n- Èric Roca Fernández\n\n# Data availability and provenance statements\n### Statement about rights\n\nThe author(s) of the manuscript have legitimate access to and permission to use the data used in this manuscript.\n\n### Summary of availability\n\n- All data is publicly available, although accessing some may require a registration.\n  - Downloading the dataset by Ashraf and Galor (2011) requires registering at OpenICPSR (free of charge, immediate registration).\n\n\n### Details on each data source\n- The paper uses the Ethnographic Atlas (Murdock, 1967).\n  We used Pat Gray's version, which includes additional variables related to climate.\n  Unfortunately, the website hosting it is no longer accessible: see [webarchive](http://web.archive.org/web/20151128133434/http://intersci.ss.uci.edu/wiki/index.php/Ethnographic_Atlas#Rdata_format_version_of_Ethnographic_Atlas).\n  More in detail, we downloaded the file linked under \"Rdata format version of Ethnographic Atlas\".\n  We provide a copy of this file.\n- The paper uses the Standard Cross-Sectional Sample (Murdock and White, 1969).\n  We use the version provided by the d-place project, accessible [here](https://github.com/D-PLACE/dplace-data/tree/master/datasets/SCCS).\n  The data is distributed under the Creative Commons Attribution 4.0 International License.\n  In particular, this script harmonizes ethnic group names.\n- The paper uses data from Eff and Abhradeep (2013).\n- The paper uses data from Ashraf and Galor (2011).\n  We use the data from the authors' [replication files](http://doi.org/10.3886/E112453V1).\n  The data is distributed under the Creative Commons Attribution 4.0 International Public License.\n  For convenience, we renamed the file `20081371_Dataset.dta` as `Ashraf_Galor__2011.dta` but, otherwise, we use it as provided.\n  Any changes made to the data are documented in the corresponding `4-Ashraf_Galor_Preparation.do`.\n- The paper uses data from Michalopoulos and Xue (2021) on folklore topics.\n  This can be downloaded from [here](https://doi.org/10.7910/DVN/IXOHKB).\n  These data is distributed under the Creative Commons 1.0 Universal Public Domain.\n  We use two files:\n  - `motifs_ea_groups.dta` to build our variable about how eclipses are explained in the folklore\n  - `concept_frequencies_ea_groups.dta` from which we retrieve the relevance of several concepts in the folklore, as well as aggregate information (number of tales and bibliography consulted).\n- The paper uses the Seshat databank as derived in Miranda and Freeman (2020).\n  These data is available [here](https://github.com/LuxMiranda/shiny-seshat).\n  The data is distributed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License.\n- The paper uses data from the Wikidata project, distributed under the Creative Commons 1.0 Universal Public Domain License.\n  Because these data changes over time, we provide the database that we retrieved at the time of writing.\n- The paper uses georeferenced information about eclipses visibility from Jubier for [solar eclipses --anular and total--](http://xjubier.free.fr/en/site_pages/solar_eclipses/5MCSE/xSE_2_Five_Millennium_Canon.html) and [lunar eclipses](http://xjubier.free.fr/en/site_pages/lunar_eclipses/5MCLE/xLE_Five_Millennium_Canon.html).\n  The data is freely available and can be used in non-commercial applications.\n- We use several datasets from Fenske (2014).\n  First, ethnic homeland characteristics (elevation, ruggedness, malaria, latitude and longitude) can be obtained [here](https://warwick.ac.uk/fac/soc/economics/staff/jefenske/FenskeETSReplication.zip?attredirects=0)\n  The data is freely available and requires a proper citation.\n  However, the underlying maps that we use were not made public and we obtained these through personal communication.\n  The files `calories_centroids.csv` and `geography.csv` are derived from these maps (`allmerge_2011_01.shp`).\n  Geography is just a dump of the database embeded in in the shapefile.\n  Calories are obtained by averaging potential caloric yields before 1500 from Ashraf and Galor (2011) in qGIS using the `zonal statistics` plugin and computing its centroids, respectively; ethnicities are renamed using the information in `merges_by_Anastasia`.\n- We employ other data sets throughout the paper:\n  - Information on cloud coverage from Wilson and Jetz (2016).\n    We used the [Mean Annual tiff](http://data.earthenv.org/cloud/MODCF_meanannual.tif) file.\n    This data is distrubuted under the Creative Commons Attribution-NonCommercial 4.0 International License.\n  - Information on lightning from the LIS/OTD 0.5 Degree High Resolution Monthly Climatology (HRMC) from [NSSTC](https://ghrc.nsstc.nasa.gov/uso/ds_docs/lis_climatology/lohrmc_dataset.html)\n    Thes data is produced by a USA Government agency and, thus, under the Public Domain.\n  - Information on volcanoes from GVP (2013), available [here](https://dx.doi.org/10.5479/si.GVP.VOTW4-2013) and [here](https://github.com/scottyhq/votw)\n    Use of the former falls under Fair Use and the latter is distributed under the GNU General Public License v3.0.\n  - Data on coastlines and rivers from [Natural Earth - Coastline v4.1.0](https://www.naturalearthdata.com/downloads/10m-physical-vectors/10m-coastline/) and [Natural Eath - Rivers v5.0.0](https://www.naturalearthdata.com/downloads/10m-physical-vectors/10m-rivers-lake-centerlines/)\n    These data are Public Domain from Kelso and Patterson (2012).\n  - Country boundaries are downloaded from the [GADM project](https://gadm.org/download_world.html)\n  - Information on fault lines from Ahlenius H. available [here.](https://github.com/fraxen/tectonicplates)\n    These data is available under the Open Data Commons Attribution License.\n  - Information on potential caloric yields from [Galor and Özak](http://dx.doi.org/10.1257/aer.20150020) available [here](https://ozak.github.io/Caloric-Suitability-Index/)\n    The data is distributed under the Creative Commons Attribution-ShareAlike 4.0 International License.\n    We use two raster files: pre1500AverageCaloriesNo0.tif and post1500AverageCaloriesNo0.tif\n  - Information on terrain altitude and ruggedness is derived from [GMTED 2010 database](https://topotools.cr.usgs.gov/gmted_viewer/gmted2010_global_grids.php)\n    Thes data is produced by a USA Government agency and, thus, under the Public Domain.\n    We use the file Mean Statistic 30-arc seconds to compute average altitude and the file Standard Dev. Statistic 30-arc seconds to compute ruggedness.\n    More in detail, for a given area, we measure ruggedness by averaging the different values of the standard deviation in altitude.\n  - Distance to Addis Abeba is computed by measuring the travelling distance from ethnic homeland centroids to the coordinates of Addis Abeba.\n    We used Özak (2010) [Human Mobility Index HMI raster](https://www.dropbox.com/s/5l8zlk81oeu1xhn/HMI.tif?dl=0) to derive optimal travel paths.\n    The data is distributed under the Creative Commons Attribution-ShareAlike 4.0 International License.\n    We forced all American ethnic groups to travel through the Bering strait.\n    Computations were made using the `r.drain` GRASS package in qGIS version 2.14.\n    For this reason, we cannot provide the replication code for this exercise but we explain below the necessary steps.\n      - First, we reprojected the original HMI to EPSG:3832 to ensure a connection between the Americas and Asia through teh Bering strait.\n      `gdalwarp -t_srs EPSG:3832 -r near -multi -of GTiff -co COMPRESS=DEFLATE -co PREDICTOR=2 -co ZLEVEL=9 HMI.tif HMI_Reprojected.tif`\n      - We create a temporary layer containing a point that represents Addis Ababa at coordinates 9.027, 38.736\n      - We use the GRASS plugin `r.cost` to compute the cumulative cost from each cell to Addis Ababa, where we assign a large cost to null cells, forcing travel over land (this step is requires plenty of memory, we cut the world in different regions and processed them separately).\n      - We used the GRASS plugin `r.drain` and the direction map created before to find the optimal route.\n\n# List of tables and figures\n\nThe provided code reproduces:\n\n- All numbers provided in text in the paper\n- All tables and figures in the paper\n\n# Description of programs and code\n\n## Instructions to generate the data and replicate the results\n\n- The provided codes expect some folders to exist.\n  Please ensure that, within the root folder (denoted `./` here) from which codes will be executed, the following folder structure exist:[^1]\n    - `./Latex/Figures/Robustness`\n    - `./Latex/Tables`\n- The code consists of two distinct parts:\n  - The creating of several datasets, including their process, is delegated to a series of Python scripts that we detail below.\n    For the purpose of replication, running these files is **not** required because we provide their outputs for convenience.\n    As a matter of fact, some datasets such as Wikidata, are subject to continuous change.\n  - Nevertheless, if the replicator wishes to fully recreate all the databases they can contact the authors.\n    Therefore, to replicate the results of the paper the replicator should rely on the provided files.\n    Four Python scripts generate Figures that appear in the paper.\n      - We run the different scripts using Python 3.10.9.\n        Required packages include pandas, geopandas, rasterio, shapely, scipy, fiona, statsmodels, seaborn as well as their dependencies.\n        These can be installed from the distribution's repository or Python PyPy repository.\n  - The data analysis and the creation of the tables and most of the figures is done in Stata with the help of the do-files provided in this replication package.\n    - To run them, *first adjust the global variable `base_path` in line 6 to point to the root of the downloaded replication folder.*\n    - The file `1-Preamble.do` is enough to replicate the entire paper, including generating the Tables and most Figures (see above for Figures generated in Python).\n      Tables are exported in LaTeX format.\n    - In this paper, we used Stata 17, running under Linux 6.3.7.\n    - The numbers referred in the text are computed within Stata and exported to a csv-like file named `exported_values.txt` inside the ./Latex folder.\n      However, we export these using a shell command (`!echo`) that is only available in Linux.\n    - We use external programs: estout, acreg (ranktest, hdfe), reghdfe (ftools), coefplot, binscatter, binsreg, xlincom, and speccurve (line 24 in `1-Preamble.do` provides precise instructions on how to install it); together with their dependencies.\n      The do-files include the code to install the required packages.\n\n## Code organization\n- The different programs are organized in folders:\n    - `Scripts` contain the Python scripts we use to process georeferenced sources and plot some results.\n      We used `Spyder` to run these scripts.\n      Because of differences between packages, notably `matplotlib`, we recommend exploring the plots within `Spyder`.[^2]\n        - `12-Create_maps.py` generates Figure 1\n        - `13-Plot_discoveries.py` generates Figure C.6.\n        - `15-Plot-robustness.py` generates Figures C.10. to C.14. (requires running the Stata files first).\n        - `14-Plot_regressions.py` generates Figures 2, C.8. and C.9. (requires running the Stata files first).\n    - `Stata` contains the Stata files that generate all the remaining Figures and all the Tables.\n\n## Running order for the replication of the results and figures in the paper\n1. Move inside the `Stata` directory\n2. Run the file `1-Preable.do`, adapting the paths as needed.\n3. Move to the `Scripts` folder: the following scripts use databases created in Stata, so it is imperative to run the Stata files before.\n4. Create the maps and some additional figures by running, adapting the paths as needed:\n  **Note for Windows users running the Python scripts**: when modifying the variable `base_path`, be sure to use forward slashes \"/\" instead of backwards slashes \"\\\\\".\n  For instance, instead of writing `C:\\Users\\` you should write `C:/Users/`.\n    1. `12-Create_maps.py`\n    2. `13-Plot_discoveries.py`\n    3. `14-Plot_regressions.py`\n    4. `15-Plot-robustness.py`\n\n# List of tables and figures\n\nThe provided code reproduces:\n\n- All numbers provided in text in the paper\n- All tables and figures in the paper\n\n## Filenames and Tables\n\n| Table       | Filename                                                                                                                                          |\n|-------------|---------------------------------------------------------------------------------------------------------------------------------------------------|\n| Table 1     | Table\\_Development.tex                                                                                                                            |\n| Table 2     | Table\\_Proxies\\_Development.tex                                                                                                                   |\n| Table 3     | Table\\_Proxies\\_Human\\_Capital.tex                                                                                                                |\n| Table 4     | Table\\_Proxies\\_Technology.tex                                                                                                                    |\n| Table 5     | Table\\_Proxies\\_Curiosity.tex                                                                                                                     |\n| Table 6     | Table\\_Religion.tex                                                                                                                               |\n| Table 7     | Table\\_Placebo.tex                                                                                                                                |\n| Table C.1   | Table\\_Development\\_15-Plot-robustness.tex                                                                                                        |\n| Table C.2   | Table\\_Proxies\\_Development\\_15-Plot-robustness.tex                                                                                               |\n| Table C.3   | Table\\_Proxies\\_Human\\_Capital\\_15-Plot-robustness.tex                                                                                            |\n| Table C.4   | Table\\_Proxies\\_Technology\\_15-Plot-robustness.tex                                                                                                |\n| Table C.5   | Table\\_Proxies\\_Curiosity\\_15-Plot-robustness.tex                                                                                                 |\n| Table C.6   | Table\\_Area.tex                                                                                                                                   |\n| Table C.7   | Table\\_Area\\_Folklore.tex                                                                                                                         |\n| Table C.8   | Table\\_Quantile.tex                                                                                                                               |\n| Table C.9   | Table\\_Int\\_1.tex                                                                                                                                 |\n| Table C.10  | Table\\_Int\\_2.tex                                                                                                                                 |\n| Table C.11  | Table\\_Int\\_3.tex                                                                                                                                 |\n| Table C.12  | Table\\_Folklore\\_Fraction.tex                                                                                                                     |\n| Table C.13  | Summary\\_Statistics.tex                                                                                                                           |\n| Figure 1    | Total\\_Eclipses.png                                                                                                                               |\n| Figure 2    | V31.pdf and V30.pdf                                                                                                                               |\n| Figure 3    | Eclipses\\_Science.pdf                                                                                                                             |\n| Figure 4    | Eclipses\\_Religion.pdf                                                                                                                            |\n| Figure C.1  | Files named Random\\_Eclipses\\_\\*.pdf[^3]                                                                                                          |\n| Figure C.2  | group\\_1.pdf                                                                                                                                      |\n| Figure C.3  | group\\_2.pdf                                                                                                                                      |\n| Figure C.4  | group\\_3.pdf                                                                                                                                      |\n| Figure C.5  | EA\\_Over\\_time.pdf and Folklore\\_Over\\_time.pdf                                                                                                   |\n| Figure C.6  | Discoveries\\_Greece.pdf and Discoveries\\_India.pdf                                                                                                |\n| Figure C.7  | eclipses\\_distribution.png                                                                                                                        |\n| Figure C.8  | V33.pdf, V66.pdf, V90.pdf, Games.pdf, Writing.pdf and Explanation.pdf                                                                             |\n| Figure C.9  | Calendar.pdf, Tasks.pdf, Technology.pdf, Thinking.pdf, Curious.pdf and Eclipse.pdf                                                                |\n| Figure C.10 | Files named 10\\_\\*.pdf                                                                                                                            |\n| Figure C.11 | Files named 25\\_\\*.pdf                                                                                                                            |\n| Figure C.12 | Files named 50\\_\\*.pdf                                                                                                                            |\n| Figure C.13 | Files named 100\\_\\*.pdf                                                                                                                           |\n| Figure C.14 | Files named 150\\_\\*.pdf                                                                                                                           |\n| Figure C.15 | Files named binsreg\\_bivariate\\_\\*.pdf and binsreg\\_multivariate\\_\\*.pdf                                                                          |\n| Figure C.16 | Science\\_binscatter\\_1.pdf, Science\\_binscatter\\_2.pdf, Science\\_binscatter\\_Limited\\_1.pdf and Science\\_binscatter\\_Limited\\_2.pdf |\n\n[^1]: Please ensure that the imputed path ends at the folder \"3 replication package\", and that the full path to this folder is copied.\n      For instance, assuming the replicator uses Windows, the global variable `base_path` in line 6 of `1-Preamble.do` could look like `C:\\Users\\YourUsername\\Downloads\\Litina_Roca-Fernandez__2019/3 replication package`\n[^2]: This may arise from differences in the rendering backend between Linux and Windows.\n[^3]: Variable v30 is the variable tracking \"Settlement Patterns\", likewise V31 corresponds to \"Population density\", v33 to \"Jurisdictional Hierarchy\", v66 to \"Class Stratification\" and v90 to \"Political Integration\".\n\n\n## Code references\n\n- Table 1:\n  - `Stata/2-EA.do` lines 7--81\n  - `Stata/3-Seshat.do` lines 74--105 and 192--214\n  - `Stata/6-Tables.do` lines 1--43\n- Table 2:\n  - `Stata/2-EA.do` lines 110--224\n  - `Stata/3-Seshat.do` lines 74--105 and 192--214\n  - `Stata/6-Tables.do` 45--89\n- Table 3:\n  - `Stata/2-EA.do` lines 110--224 and lines 231--275\n  - `Stata/3-Seshat.do` lines 74--105\n  - `Stata/6-Tables.do` lines 91--138\n- Table 4:\n  - `Stata/2-EA.do` lines 110-224\n  - `Stata/3-Seshat.do` lines 74--105\n  - `Stata/4-Ashraf_Galor.do`\n  - `Stata/6-Tables.do` lines 140--183\n- Table 5:\n  - `Stata/2-EA.do` lines 231--275\n  - `Stata/5-Wikipedia.do` lines 7--25\n  - `Stata/6-Tables.do` lines 185--228\n- Table 6:\n  - `Stata/2-EA.do` lines 278--283 and lines 231--275\n  - `Stata/5-Wikipedia.do` lines 7--25\n  - `Stata/6-Tables.do` 230--272\n- Table 7:\n  - `Stata/2-EA.do` lines 231--275\n  - `Stata/6-Tables.do` lines 274--308\n- Table C.1:\n  - `Stata/2-EA_robustness.do` lines 2--24\n  - `Stata/3-Seshat_robustness.do` lines 80--111\n  - `Stata/6-Tables_robustness.do` lines 1--56\n- Table C.2:\n  - `Stata/2-EA_robustness.do` lines 2--24\n  - `Stata/3-Seshat_robustness.do` lines 80-111\n  - `Stata/6-Tables_robustness.do` lines 58--115\n- Table C.3:\n  - `Stata/2-EA_robustness.do` lines 2--24 and lines 29--45\n  - `Stata/3-Seshat_robustness.do` lines 80-111\n  - `Stata/6-Tables_robustness.do` lines 117--174\n- Table C.4:\n  - `Stata/2-EA_robustness.do` lines 2--24\n  - `Stata/3-Seshat_robustness.do` lines 80-111\n  - `Stata/4-Ashraf_Galor_robustness.do`\n  - `Stata/6-Tables_robustness.do` lines 176--233\n- Table C.5:\n  - `Stata/2-EA_robustness.do` lines 29--45\n  - `Stata/5-Wikidata_robustness.do`\n  - `Stata/6-Tables_robustness.do` lines 235--294\n- Table C.6:\n  - `Stata/2-EA_Area.do` lines 8--95\n  - `Stata/6-Tables_Area.do` lines 1--70\n- Table C.7:\n  - `Stata/2-EA_Area.do` lines 8--95\n  - `Stata/6-Tables_Area.do` lines 72--142\n- Table C.8:\n  - `Stata/2-EA_robustness.do` lines 49--63\n  - `Stata/6-Tables_Area.do` lines 335--368\n- Table C.9:\n  - `Stata/2-EA_Area.do` lines 106--130\n  - `Stata/6-Tables_Area.do` lines 145--209\n- Table C.10:\n  - `Stata/2-EA_Area.do` lines 106--130\n  - `Stata/6-Tables_Area.do` lines 211--286\n- Table C.11:\n  - `Stata/3-Seshat_robustness.do` lines 122--180\n  - `Stata/6-Tables_Area.do` lines 288--341\n- Table C.12:\n  - `Stata/2-EA_robustness.do` lines 29--45\n  - `Stata/6-Tables_robustness.do` lines 296--329\n- Figure 1:\n  - `Scripts/12-Create_maps.py`\n- Figure 2:\n  - `Scripts/14-Plot_regressions.py`\n- Figure 3:\n  - `Stata/5-Wikipedia.do` lines 34--67\n- Figure 4:\n  - `Stata/5-Wikipedia.do` lines 70--103\n- Figure C.1:\n  - `Stata/2-EA_robustness.do` lines 177--235\n- Figure C.2:\n  - `Stata/2-EA_Speccurve.do`\n- Figure C.3:\n  - `Stata/2-EA_Speccurve.do`\n- Figure C.4:\n  - `Stata/2-EA_Speccurve.do`\n- Figure C.5:\n  - `Stata/2-EA_Over_time.do`\n- Figure C.6:\n    - `Scripts/13-Plot_discoveries.py`\n- Figure C.7:\n    - `Scripts/7-Compute_eclipse_incidence.py`\n- Figure C.8:\n    - `Scripts/14-Plot_regressions.py`\n- Figure C.9:\n    - `Scripts/14-Plot_regressions.py`\n- Figure C.10--C.14:\n    - `Scripts/15-Plot-robustness.py`\n- Figure C.15:\n    - `Stata/2-EA-robustness.do` lines 151--165\n- Figure C.16:\n    - `Stata/5-Wikipedia.do` lines 109--127\n\n# References\n- Ahlenius, H. (2014). World tectonic plates and boundaries. [Github repository](https://github.com/fraxen/tectonicplates#readme)\n- Ashraf, Q. and Galor, O. (2011). Dynamics and stagnation in the Malthusian epoch. American Economic Review, 101(5):2003–2041. [Data](http://doi.org/10.3886/E112453V1)\n- Danielson, J.J., and Gesch, D.B. (2011), Global multi-resolution terrain elevation data 2010 (GMTED2010): U.S. Geological Survey Open-File Report 2011–1073, 26 p. [Data](https://topotools.cr.usgs.gov/gmted_viewer/gmted2010_global_grids.php)\n- Eff, E. and Maiti, A. (2013). A measure of technological level for the Standard Cross-Cultural Sample. Working Papers 201302, Middle Tennessee State University, Department of Economics and Finance. The data appears in the paper.\n- Fenske, J. (2014). Ecology, trade, and states in pre-colonial Africa. Journal of the European Economic Association, 12(3):612–640. [Data](https://warwick.ac.uk/fac/soc/economics/staff/jefenske/FenskeETSReplication.zip?attredirects=0)\n- Galor O. and Özak, Ö. (2016). The Agricultural Origins of Time Preference, American Economic Review, 106(10): 3064--3103. [Data](https://ozak.github.io/Caloric-Suitability-Index/)\n- GADM (2019). GADM maps and data. [Data](https://gadm.org/download_world.html)\n- Henderson, C. (2018). Demonstration of Jupyter notebook deployed with Binder. [Github repository](https://github.com/scottyhq/votw)\n- The v2.2 gridded satellite lightning data were produced by the NASA LIS/OTD Science Team and are available from the [Global Hydrology Resource Center](https://ghrc.nsstc.nasa.gov/uso/ds_docs/lis_climatology/lohrmc_dataset.html)\n- Jubier, X. M. (2019). [Solar eclipses](http://xjubier.free.fr/en/site_pages/Solar_ Eclipses.html) Accessed: 2019\n- Jubier, X. M. (2019). [Lunar eclipses](http://xjubier.free.fr/en/site_pages/Lunar_ Eclipses.html). Accessed: 2019\n- Kathryn R. Kirby, Russell D. Gray, Simon J. Greenhill, Fiona M. Jordan, Stephanie Gomes-Ng, Hans-Jörg Bibiko, Damián E. Blasi, Carlos A. Botero, Claire Bowern, Carol R. Ember, Dan Leehr, Bobbi S. Low, Joe McCarter, William Divale, & Michael C. Gavin. (2021). D-PLACE/dplace-data: D-PLACE – the Database of Places, Language, Culture and Environment (v2.2.1) [Data set]. [Zenodo.](https://doi.org/10.5281/zenodo.5554395)\n- Kelso N. V. and Patterson T. (2012), Natural Earth. [Data set]. [Natural Earth](https://www.naturalearthdata.com/downloads/10m-physical-vectors/)\n- Michalopoulos, S. and Xue, M. M. (2021). Folklore. The Quarterly Journal of Economics, 136(4):1993–2046. [Data](https://doi.org/10.7910/DVN/IXOHKB)\n- Miranda L. and Freeman J. (2020). The two types of society: Computationally revealing recurrent social formations and their evolutionary trajectories. PLoS ONE, 15(5):e0232609. [Data set]: [Seshat](https://github.com/LuxMiranda/shiny-seshat) Accessed: 2019\n- Murdock, G. P. (1967). Ethnographic atlas: A summary. Ethnology, 6(2):109–236. Downloaded from [Archive](http://web.archive.org/web/20151128133434/http://intersci.ss.uci.edu/wiki/index.php/Ethnographic_Atlas#Rdata_format_version_of_Ethnographic_Atlas)\n- Murdock, G. P. and White, D. R. (1969). Standard cross-cultural sample. Ethnology, 8(4):329–369.\n- Global Volcanism Program, 2022. [Database] Volcanoes of the World (v. 4.6.7; 20 Mar 2018). Distributed by Smithsonian Institution, compiled by Venzke, E. [10.5479/si.GVP.VOTW5-2023.5.1](https://doi.org/10.5479/si.GVP.VOTW5-2023.5.1)\n- Özak, Ömer (2010). The voyage of homo-economicus: Some economic measures of distance. Department of Economics, Brown University. [Data](https://www.dropbox.com/s/5l8zlk81oeu1xhn/HMI.tif?dl=0)\n- Venzke, E., editor (2013). Global Volcanism Program. Smithsonian Institution.\n- Wilson, A. M. and Jetz, W. (2016). Remotely sensed high-resolution global cloud dynamics for predicting ecosystem and biodiversity distributions. PLOS Biology, 14(3):e1002415. [Data](http://data.earthenv.org/cloud/MODCF_meanannual.tif)\n- Wikidata - A Collaborative Knowledge Base. [Wikidata](https://www.wikidata.org)",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Preparing the README</span>"
    ]
  },
  {
    "objectID": "folder_organization.html",
    "href": "folder_organization.html",
    "title": "4  Folder Organization and Data Management",
    "section": "",
    "text": "4.1 Why Project Organization Matters\nA well-organized project is the foundation of reproducible research. Good structure makes your life easier, helps collaborators, and is now required by top journals. It also prevents disasters—like losing track of which file is which, or accidentally overwriting your raw data.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Folder Organization and Data Management</span>"
    ]
  },
  {
    "objectID": "folder_organization.html#recommended-folder-structure",
    "href": "folder_organization.html#recommended-folder-structure",
    "title": "4  Folder Organization and Data Management",
    "section": "4.2 Recommended Folder Structure",
    "text": "4.2 Recommended Folder Structure\nHere’s a robust structure for economic replication projects:\n├── README.md\n├── data\n│   ├── original      # Raw, immutable data (never edit!)\n│   └── temporary     # Cleaned or processed data (can be regenerated)\n├── code\n│   ├── analysis      # Scripts for data cleaning, analysis, etc.\n│   └── figures       # Scripts for generating plots\n├── paper\n│   ├── figures       # Auto-generated figures\n│   ├── tables        # Auto-generated tables\n│   └── paper.tex     # Manuscript\n\nREADME.md: The project’s roadmap. Document everything here—see Preparing the README.\ndata/original: Sacred ground. Store only raw, downloaded data here. Never modify these files. Make them read-only if possible.\ndata/temporary: For cleaned, merged, or processed data. These files can always be recreated from code.\ncode/: All scripts, organized by purpose or language.\npaper/: Manuscript and all outputs (figures, tables) generated by code.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Folder Organization and Data Management</span>"
    ]
  },
  {
    "objectID": "folder_organization.html#golden-rules-for-data-management",
    "href": "folder_organization.html#golden-rules-for-data-management",
    "title": "4  Folder Organization and Data Management",
    "section": "4.3 Golden Rules for Data Management",
    "text": "4.3 Golden Rules for Data Management\n\nNever modify original data.\nAll cleaning and processing must be done in code, with outputs saved to data/temporary. This guarantees you can always start from scratch. Give each dataset a meaningful name that reflects its content and purpose.\n\n\n\nNaming conventions to avoid\n\n\nDocument everything.\nFor every dataset, record the source, download date, license, access conditions, and any modifications in the README. This is required by journals—see Preparing the README.\nUse open formats.\nPrefer CSV, TXT, or other open formats. Avoid proprietary formats (e.g., Stata .dta, Excel .xlsx) when possible:\n\nOpen formats are readable by anyone, anywhere, at any time.\nProprietary formats change (e.g., Stata .dta files are not always backward compatible).\nExcel can mangle data (see the UK COVID case undercount or Lehman Brothers asset deal error).\n\nSome journals now require open formats for replication packages.\nBack up your work.\nNever rely on a single device. Use external drives or institutional remote storage for data and code.\n\n\n\n\n\n\n\nCaution\n\n\n\nOne common mistake is to believe that working with a Dropbox or Google Drive folder is already a backup solution. The problem with these services is that they sync the data. If you accidentally delete a file or it gets corrupted, it may be lost forever, even in online copy. It is best to use backup solutions that offer snapshotting or versioning, so you can recover previous versions of files: e.g. Back-in-Time, Borg, Duplicati.\n\n\n\nVersion control for code, not data.\nUse Git for code, scripts, and text files. Don’t put large data files in Git—it’s not designed for them. Use .gitignore to exclude data and outputs. For very large files, tools like Git LFS exist, but keep it simple unless necessary. We will cover Git usage in detail in a later section.\nAvoid multiple versions of the same file.\nDon’t create final_v2, final_v3, etc. as this leads to confusion and makes it hard to track changes. Use Git to track changes and revert if needed.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Folder Organization and Data Management</span>"
    ]
  },
  {
    "objectID": "folder_organization.html#intermediate-outputs-when-and-how",
    "href": "folder_organization.html#intermediate-outputs-when-and-how",
    "title": "4  Folder Organization and Data Management",
    "section": "4.4 Intermediate Outputs: When and How",
    "text": "4.4 Intermediate Outputs: When and How\nSometimes, processing data is slow (e.g., merging huge files, running simulations). In these cases, it’s practical to save intermediate results in data/temporary.\nHowever:\n\nIntermediate files should not be distributed in the replication package unless absolutely necessary (e.g., if processing takes hours/days).\nIf you do include them, explain in the README why, and provide code to regenerate them if possible.\nIf the original data or processing code changes, you must update all intermediate files.\nAdvanced: Consider using a Makefile to automate dependencies. This ensures that if a source file changes, all downstream files are rebuilt. (A Makefile is a script that specifies how to build outputs from inputs, and only reruns steps when needed. This is advanced but powerful.)",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Folder Organization and Data Management</span>"
    ]
  },
  {
    "objectID": "folder_organization.html#practical-data-issues",
    "href": "folder_organization.html#practical-data-issues",
    "title": "4  Folder Organization and Data Management",
    "section": "4.5 Practical Data Issues",
    "text": "4.5 Practical Data Issues\n\nNon-data entries: Some sources (e.g., World Bank) may include images, notes, or metadata in their downloads. Always inspect files and write code to remove non-data content.\nMerging difficulties: Merging datasets can be tricky due to inconsistent IDs, typos, or missing values.\nDatetime formats: Dates are a common source of errors (e.g., Excel’s date handling). Always check and standardize date formats.\nEncoding issues: Files may use different character encodings (UTF-8, Latin-1, etc.), leading to errors or garbled text. Always specify encoding when reading files (e.g., encoding='utf-8' in Python or R).",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Folder Organization and Data Management</span>"
    ]
  },
  {
    "objectID": "folder_organization.html#issues-with-the-data",
    "href": "folder_organization.html#issues-with-the-data",
    "title": "4  Folder Organization and Data Management",
    "section": "4.6 Issues with the Data",
    "text": "4.6 Issues with the Data\nWorking with standalone data is rare. In general, researchers must combine different datasets to create a comprehensive analysis. Every time you merge datasets, problems can arise. In fact, it is quite common to spend more time on harmonizing datasets than on the actual analysis.\n\nMerging on country identifiers:\n\nCountries appear (e.g. South Sudan, Eritrea) and disappear (e.g., Yugoslavia, Czechoslovakia) over time.\n\nDifferent datasets may use different country names or codes (e.g., “USA” vs “United States”).\n\n\nDatetime issues: Dates may be recorded in different formats (e.g., “YYYY-MM-DD” vs “MM/DD/YYYY”), be recorded as strings (text) instead of dates, use different languages for months, etc.\nEncoding issues: Files may use different character encodings leading to errors or garbled text. This is specially common when names include accents or special characters.\n\nExample from my own research: harmonize more than 2000 town names for Belgium because, French,\n\n\n\n\n\n\n\n\nTip\n\n\n\nAfter each merge, check which IDs are present in the merged dataset and which are not. This can help identify any issues with the merge process.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWhen merging in Stata, if the master dataset already has a variable with the same name as one in the using dataset, Stata will not update the observations for which there is match, see the Stata manual.\nOriginal Data               Updated Data               Merge Result\n+---------+------------+   +---------+------------+    +---------+------------+\n| Country | Population |   | Country | Population |    | Country | Population |\n+---------+------------+   +---------+------------+    +---------+------------+\n| France  |         60 |   | France  |         65 |    | France  |         60 |\n| UK      |         70 |   | UK      |         80 |    | UK      |         70 |\n| Italy   |            |   | Italy   |         65 |    | Italy   |         65 |\n+---------+------------+   +---------+------------+    +---------+------------+",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Folder Organization and Data Management</span>"
    ]
  },
  {
    "objectID": "folder_organization.html#example-downloading-and-organizing-world-bank-data",
    "href": "folder_organization.html#example-downloading-and-organizing-world-bank-data",
    "title": "4  Folder Organization and Data Management",
    "section": "4.7 Example: Downloading and Organizing World Bank Data",
    "text": "4.7 Example: Downloading and Organizing World Bank Data\nSuppose you’re replicating a paper using World Bank data:\n\nDownload the CSV from the World Bank site.\nSave it as data/original/world_bank_data.csv.\nImmediately record the download date, source URL, and license in your README.\nWrite a script to clean the data and save the result as data/temporary/cleaned_data.csv.\nIf the cleaning takes a long time, you may save an intermediate file, but document this and provide code to regenerate it.\n\nKey: Anyone should be able to delete data/temporary, rerun your code, and get identical results.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Folder Organization and Data Management</span>"
    ]
  },
  {
    "objectID": "stata-replication.html",
    "href": "stata-replication.html",
    "title": "5  Replication with Stata: Best Practices",
    "section": "",
    "text": "5.1 Stata: Setting Up for Reproducibility\nStata is a powerful tool for empirical research, but reproducibility requires discipline and structure. Here’s how to set up your project for robust, transparent replication.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Replication with Stata: Best Practices</span>"
    ]
  },
  {
    "objectID": "stata-replication.html#stata-setting-up-for-reproducibility",
    "href": "stata-replication.html#stata-setting-up-for-reproducibility",
    "title": "5  Replication with Stata: Best Practices",
    "section": "",
    "text": "5.1.1 1. Use Global Variables for Paths\nDefine all key directories at the top of your main do-file. This makes your code portable and easy to maintain.\nglobal project_path \"/path/to/your/project\"\nglobal data_original \"$project_path/data/original\"\nglobal data_temp     \"$project_path/data/temporary\"\nglobal code          \"$project_path/code\"\nglobal output        \"$project_path/paper\"\nglobal ado           \"$project_path/ado\"\n\n\n\n\n\n\nTip\n\n\n\nNever hardcode paths in your scripts. Use these globals everywhere. Indicate in the README where to set these paths and what to adapt.\n\n\n\n\n5.1.2 2. Managing Ado Packages for Replication\nIt is quite common to add new capabilities to Stata via user-written packages (ado files), for instance, the estout, ivreg2 or reghdfe. These packages are updated and may lead to changes in your results. Ideally, we would indicate somewhere which versions of these packages were used, but this is only being slowly worked on as an ado package. We can circumvent this issue by storing ado files within a project-specific folder. By doing so, we can update them as needed and bundle this folder with our replication package.1\n\nCreate a project-specific ado folder (e.g., ado/, see above).\nInstall all required packages there in your master file.\n\nComment out ssc install lines to avoid checking packages at every run.\n\nBundle this ado folder with your replication package so users get the exact versions you used.\n\n// Create a project-specific ado directory\nsysdir set PLUS \"$ado\"\n\n// Install packages into this folder\nssc install estout, replace\n\n\n\n\n\n\nImportant\n\n\n\nMake sure to change the sysdir settings in your scripts to point to the correct ado folder. For instance, in your master file, add\nsysdir set PLUS \"$ado\"\n\n\n\n\n\n\n\n\nTip\n\n\n\nDo not include the ssc install lines in the replication package you share to avoid users accidentally updating packages and breaking reproducibility. Instead, include the ado folder itself.\n\n\n\n\n5.1.3 3. Comment Generously\n\nExplain your logic, not just the syntax.\nComments help your future self and others understand why you did something.\nDocument assumptions, data quirks, and any manual steps.\n\n/*\n    Main regressions on population density and settlement patterns\n    Add covariates little by little\n*/\nforeach v of global outcomes_ea {\n    reg `v' $EA1\n    eststo model_`v'_ea1\n    reg `v' $EA2\n    eststo model_`v'_ea2\n}\n\n\n5.1.4 4. Write Reusable Code: Functions and Loops\n\nEncapsulate repetitive tasks in programs (functions).\nUse loops to iterate over variables, controls, or specifications.\nThis makes it easy to add/remove variables or change specifications later.\n\nIn your main do-file, indicate groups of variables that you reuse together.\n/******************************************************\n* Sets of controls\n******************************************************/\n\nglobal myreg            \"eclipses_log\";\nglobal distances        \"distance_coast_km_log distance_addis_km_log distance_river_km_log \n                         distance_volcano_km_log distance_tectonic_km_log\";\nglobal geography        \"rugavg elevavg malaravg calavg abs_lat south i.mht_enc\";\nglobal area             \"area_km\";\nglobal motifs           \"eclipse_related calendar_related thinking_related cloud_related \n                         lightning_related rock_related sand_related white_related purple_related \n                         curious_related\";\nglobal motifs_religion  \"religion_related pray_related religious_related\";\n\n/******************************************************\n* Sets of dependent variables\n******************************************************/\n\nglobal outcomes_ea_3            \"v31 v30\";\nglobal outcomes_ea_1            \"v33 v66 v90 tasks technology strategy writing explanation\";\nThe following is a relatively complex Stata program that determines the appropriate regression model based on the outcome variable. It was useful to avoid forgetting control variables and ensuring the correct model specification. Then, if regressions with v90 were to be run under OLS, it would be enough to modify the global macro ols_regs and probit_regs.\n/******************************************************\n* Programs\n******************************************************/\n\n/* Determine regression model and adjust it */\n#delimit ;\ncapture program drop reg_model;\nprogram reg_model, rclass;\n    noisily di \"`1'\";\n   /* Pass the outcome variable as argument */\n\n   /* The sample should be limited when using V90, values 0 and 8 are for missing */\n    if \"`1'\" == \"v90\" {;\n        return local extras \"if v90&gt;0&v90&lt;8\";\n    };\n    else if (strpos(\"$outcomes_seshat_1\", \"`1'\") &gt; 0 | strpos(\"$outcomes_seshat_3\", \"`1'\") &gt; 0) & \"`1'\" != \"writing\" {;\n            return local extras \"if sd &gt; 0\";\n    };\n    else {;\n        return local extras \" \";\n    };\n\n    /* When the outcome is the number of tasks, control for the number of surveyed tasks */\n    if \"`1'\" == \"tasks\" {;\n        return local add_regressor \"tasks_with_info\";\n    };\n\n    /* When outcomes are from the folklore database we add as controls\n         the number of published books\n         the year of first publication\n         the number of motifs\n       Additionally, we clean the variables from terms that appear as related\n       in ConceptNet\n    */\n    else if strpos(\"$outcomes_folklore\", \"`1'\") {;\n        local add_regressor \"lnnmbr_title lnyear_firstpub lnmotifs_total\";\n        /* Use the global folklore_controls to determine the control we should add\n           It is organized as a dictionary.\n           First, we retrieve the position of the look-up word\n           The next entry is the value associated to the searched key, so we add 1 to the previous index\n       */\n       if strpos(\"$folklore_controls\", \"`1'\") {;\n            local motif_position : list posof \"`1'\" in global(folklore_controls);\n            local motif_position = `motif_position' + 1;\n            local control : word `motif_position' of $folklore_controls;\n            local add_regressor \"`add_regressor' `control'\";\n        };\n        return local add_regressor `add_regressor';\n    };\n    /* In the regressions using the Seshat, we control for total population throughout\n       except (obviously) when the outcome variable is population density\n   */\n   else if strpos(\"$outcomes_seshat_1\", \"`1'\") &gt; 0 | strpos(\"$outcomes_seshat_3\", \"`1'\") &gt; 0 {;\n        /* The previous expression also matches the Ethnographic Atlas variable `writing`.\n           We create an ad-hoc rule to avoid adding `p_polity_population` to its regression.\n        */\n        if \"`1'\" == \"writing\" {;\n            return local add_regressor \" \";\n        };\n        else if \"`1'\" == \"density\" {;\n            return local add_regressor \" \";\n        };\n        else {;\n            return local add_regressor \"polity_population\";\n        };\n    };\n    else {;\n        return local add_regressor \" \";\n    };\n\n    /* Select the appropriate model */\n    if strpos(\"$oprobit_regs\", \"`1'\") {;\n        return local method \"oprobit\";\n        return local limit \"iter(100)\";\n    };\n    if strpos(\"$ols_regs\", \"`1'\") {;\n        return local method \"reg\";\n        return local limit \" \";\n    };\n    if strpos(\"$nbreg_regs\", \"`1'\") {;\n        return local method \"nbreg\";\n        return local limit \"iter(100)\";\n    };\n    if strpos(\"$poisson_regs\", \"`1'\") {;\n        return local method \"poisson\";\n        return local limit \"iter(100)\";\n    };\n    if strpos(\"$probit_regs\", \"`1'\") {;\n        return local method \"probit\";\n        return local limit \"iter(100)\";\n    };\nend;\n\n/* Add additional information to the regressions */\n#delimit ;\ncapture program drop add_information;\nprogram add_information;\n    /* Pass the name of the locals and their values as arguments \n       Ex: add_information local_1 value_1 local_2 value_2 \n    */\n    tokenize `0';\n    local n : word count `0';\n    if mod(`n', 2) != 0 {;\n        di \"You should enter the information as local_1 value_1 local_2 value_2\";\n        exit;\n    };\n    forval i = 1/`n' {;\n        if mod(`i', 2) == 1 {;\n            local next = `i' + 1;\n            qui estadd local ``i'' \"``next''\";\n        };\n        else {;\n            continue;\n        };\n    };\nend;\nNow you can loop over outcomes and always use the right model and controls:\nforeach v of global outcomes {\n    reg_model \"`v'\"\n    local controls = r(controls)\n    local method = r(method)\n    `method' `v' `controls'\n}\n\n\n5.1.5 5. Separate Analysis and Table Generation\n\nFirst, generate all results and store them (e.g., with eststo).\n\nThere is a limit in the number of stored estimates, I have hit it.\n\nThen, in a separate do-file, generate all tables and figures.\nThis keeps your workflow modular and easier to debug.\n\n\n\n5.1.6 6. General Workflow Example\n\nSet up paths and globals at the top of your master do-file.\nInstall dependencies (once, not in the replication package).\n\n/*\n  1-Master.do\n  This file sets up the environment, loads data, and runs the main analysis.\n  Adjust paths and globals as needed.\n*/\n\n// Paths\nglobal project_path \"/path/to/your/project\"\nglobal data_original \"$project_path/data/original\"\nglobal data_temp     \"$project_path/data/temporary\"\nglobal code          \"$project_path/code\"\nglobal output        \"$project_path/paper\"\nglobal ado           \"$project_path/ado\"\n\n// Variables for analysis\nglobal EA1 \"v1 v2 v3\"\nglobal EA2 \"v4 v5 v6\"\n\n// Ado directory\nsysdir set PLUS \"$ado\"\n\n// Install dependencies\nssc install estout\n\n\n// Analysis\ndo \"$code/2-Data_cleaning.do\"\ndo \"$code/3-Analysis.do\"\ndo \"$code/4-Results.do\"\n\nPrepare data: load, clean, and save to data/temporary.\n\n/*\n  2-Data_cleaning.do\n  This file handles data loading, cleaning, and preparation.\n*/\n\nuse \"$data_original/your_data.dta\", clear\n\n* Document your data cleaning steps here\nsave \"$data_temp/cleaned_data.dta\", replace\n\nRun analysis: loop over outcomes, use functions for model selection, store results.\n\n/*\n  3-Analysis.do\n  This file contains the main analysis code.\n*/\n\n// Main regressions, loop over outcomes\nforeach v of global outcomes {\n    reg_model \"`v'\"\n    local controls = r(controls)\n    local method = r(method)\n    `method' `v' `controls'\n    eststo `method'_`v'\n}\n\nGenerate tables/figures: in a dedicated script, using stored results. The tables (and figures) should never be edited manually. If you need to make changes, update the code and re-run the analysis. This ensures reproducibility and reduces errors.\n\n/*\n  4-Results.do\n  This file generates tables and figures from the stored results.\n*/\n\nlocal table_1 \"\"\nforeach v of global outcomes {\n    forval i=0/2 {\n        local table_1 \"`table_1' `v'_r`i'\"\n        }\n    }\nforeach v of global outcomes_seshat {\n    forval i=1/2 {\n        local table_1 \"`table_1' `v'_`i'\"\n        }\n    }\n\n#delimit ;\nestout `table_1'\n    using \"${path_tables}/Table_Development.tex\",\n    cells(\"b(fmt(3))\" \"se(fmt(3) par star)\" \"conleyse(fmt(3) par([ ]) star pval(conleyp))\")\n    starlevels(`\"\\sym{*}\"' 0.1 `\"\\sym{**}\"' 0.05 `\"\\sym{***}\"' 0.01, label(\" \\(p&lt;@\\)\"))\n    varwidth(10)\n    modelwidth(9)\n    delimiter(&)\n    end(\\\\)\n    prehead(&\\multicolumn{6}{c}{Ethnographic Atlas}&\\multicolumn{2}{c}{Seshat}\\\\\n            \\cmidrule(lr){2-7}\\cmidrule(lr){8-9})\n    posthead(\"\\midrule\")\n    prefoot(\"\\midrule\")\n    mgroups(\"Population Density\" \"Settlement Patterns\" \"Population Density\",  \n        pattern(1 0 0 1 0 0 1 0) prefix(\\multicolumn{@span}{c}{) suffix(}) span erepeat(\\cmidrule(lr){@span})) \n    mlabels(none)\n    varlabels($labels)\n    numbers(\\multicolumn{@span}{c}{( )})\n    collabels(none)\n    eqlabels(, begin(\"\\midrule\" \"\") none)\n    substitute(_ \\_ \"\\_cons \" \\_cons)\n    interaction(\" $\\times$ \")\n    level(95)\n    style(esttab)\n        rename(eclipses eclipses_log)\n    replace\n    keep(eclipses_log distance_volcano_km_log distance_tectonic_km_log volcanoes)\n    order(eclipses_log distance_volcano_km_log distance_tectonic_km_log volcanoes)\n    stats(fe tim geo ethnic con r2_p N, fmt(0 0 0 0 0 3 0)\n        layout(\"\\multicolumn{1}{c}{@}\" \"\\multicolumn{1}{c}{@}\" \"\\multicolumn{1}{c}{@}\" \"\\multicolumn{1}{c}{@}\" \"\\multicolumn{1}{c}{@}\" \"\\multicolumn{1}{S}{@}\" \"\\multicolumn{1}{c}{@}\")\n        labels(\"Fixed effects\" \"Time Fixed Effects\" \"Geography\" \"Ethnic\" \"Controls Seshat\" \"\\midrule \\(R^{2}\\)/Pseudo-\\(R^{2}\\)\" \"Observations\"));\n#delimit cr",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Replication with Stata: Best Practices</span>"
    ]
  },
  {
    "objectID": "stata-replication.html#latex",
    "href": "stata-replication.html#latex",
    "title": "5  Replication with Stata: Best Practices",
    "section": "5.2 Latex",
    "text": "5.2 Latex\nWith a Stata code that generates readily usable LaTeX tables and figures, you can easily integrate your results into your academic papers. Again, the tables (and figures) should never be edited manually. If you need to make changes, update the code and re-run the analysis. This ensures reproducibility and reduces errors. For instance, do not export tables as Excel or Word files to make formatting changes. Integrating the results in LaTeX is straightforward with the right Stata commands.\nTable \\ref{tab:development} presents the results of regressions using the Ethnographic Atlas and Seshat datasets.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Determinants of Population Density and Settlement Patterns}\n\\label{tab:development}\n\\begin{tabular}{l*{6}{c}}\n\\input{tables/Table_Development.tex}\n\\end{tabular}\n\\tablenotes{\n    Notes: This table presents the results of the main regressions for each dataset. (...)\n}\n\\end{table}\nHere, the \\input{tables/Table_Development.tex} command directly includes the LaTeX code generated by Stata, ensuring that your tables are always up-to-date with your latest analysis. \\tablenotes is a personal Latex macro for adding table-specific notes.\nHowever, another challenge remains: referring to specific values in the table. For instance, you might want to reference the coefficient for population density in the Ethnographic Atlas dataset. While it is possible to automate this, this is not straightforward in Stata. For example, it is possible to export the results to a CSV file, refer to them with placeholders and use a script to replace the placeholders with the actual values. As we will see in the next section, this is much easier in Quarto.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Replication with Stata: Best Practices</span>"
    ]
  },
  {
    "objectID": "stata-replication.html#footnotes",
    "href": "stata-replication.html#footnotes",
    "title": "5  Replication with Stata: Best Practices",
    "section": "",
    "text": "Check whether ado packages are redistributable.↩︎",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Replication with Stata: Best Practices</span>"
    ]
  },
  {
    "objectID": "python-replication.html",
    "href": "python-replication.html",
    "title": "Replication Example: Python + Quarto",
    "section": "",
    "text": "6.1 Why Python + Quarto?\nPython is a flexible, powerful language for data analysis, and Quarto lets you combine code, text, and outputs in a single, reproducible document. This is ideal for replication: your analysis, results, and explanations are all in one place and can be re-run with a single command.\nAdvantages: - All-in-one: Combine code, results, and narrative. - Reproducible: Anyone can re-run the entire analysis and get the same results. - Transparent: Every step is documented and visible. - Flexible: Mix Python with R, Julia, or bash if needed.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Replication with Python and Quarto: Best Practices</span>"
    ]
  },
  {
    "objectID": "python-replication.html#project-setup-and-best-practices",
    "href": "python-replication.html#project-setup-and-best-practices",
    "title": "Replication Example: Python + Quarto",
    "section": "6.2 Project Setup and Best Practices",
    "text": "6.2 Project Setup and Best Practices\nAs with Stata, follow a clear folder structure (see Folder organization).\nKeep original data in data/original/ and never modify it.\nSave processed data in data/temporary/ if needed.\n\nDocument your Python version and Quarto version, including dependencies like pandas or statsmodels, in the README.\n\nIt is also possible to pin your dependencies using tools like poetry and python environments. We are not explaining it here, though.\n\n\nIf your analysis is slow or computationally intensive:\n\nUse Quarto code chunk options like eval: false to skip running heavy code when updating text. More on this later.\nYou can cache results with Quarto’s chunk caching (cache: true), but always provide code to regenerate everything from scratch.\nIf you must include pre-computed intermediate data, explain this in the README and allow users to re-run the full analysis if desired.\n\n\n\n\n\n\n\nNote\n\n\n\nYou can export tables and figures from Python (e.g., as .tex or .png) and include them in your LaTeX manuscript. With Quarto, you can also render the entire document to PDF or HTML, ensuring all results are up-to-date and reproducible.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Replication with Python and Quarto: Best Practices</span>"
    ]
  },
  {
    "objectID": "python-replication.html#example-fully-reproducible-analysis-in-quarto",
    "href": "python-replication.html#example-fully-reproducible-analysis-in-quarto",
    "title": "Replication Example: Python + Quarto",
    "section": "6.3 Example: Fully Reproducible Analysis in Quarto",
    "text": "6.3 Example: Fully Reproducible Analysis in Quarto\nBelow is a minimal example of a Quarto document for a replication package.\nThis workflow loads data, cleans it, runs regressions, and exports a LaTeX table—all in one go. We use data from a public URL for demonstration.\nWhen writing the code, you can always check it on a console before integrating it in your Quarto document.\n---\ntitle: \"Replication Example: Python + Quarto\"\n---\n\n\n```{python}\n#| echo: false\n\nimport pandas as pd\n#import statsmodels.api as sm\n\n# Data URLS\ndata_url = 'https://raw.githubusercontent.com/allisonhorst/palmerpenguins/refs/heads/main/inst/extdata/penguins.csv'\n\ndf = pd.read_csv(data_url)\n\n# Create a summary grouping by species\ngrouped_data = df.groupby('species').agg({\n    'bill_length_mm': 'mean',\n    'bill_depth_mm': 'mean',\n    'flipper_length_mm': 'mean',\n    'body_mass_g': 'mean'\n}).reset_index().round(1).rename(columns = {\n    'species' : 'Species',\n    'bill_length_mm': 'Bill Length (mm)',\n    'bill_depth_mm': 'Bill Depth (mm)',\n    'flipper_length_mm': 'Flipper Length (mm)',\n    'body_mass_g': 'Body Mass (g)'\n})\n```\n\nThe data contains measurements of penguin species, including bill length, bill depth, flipper length, and body mass.\n@tbl-summary provides summary statistics for these measurements by species.\n\n```{python}\n#| echo: false\n#| tbl-cap: Summary statistics by species\n#| label: tbl-summary\n\ngrouped_data\n```\n\n\nWe can also visualize the relationship between bill length and body mass, as depicted in @fig-scatter.\n\n```{python}\n#| echo: false\n#| fig-cap: Scatter plot of bill length vs. body mass\n#| label: fig-scatter\n\nimport seaborn as sns\nscatter = sns.scatterplot(x='bill_length_mm', y='body_mass_g', data=df, hue='species')\nscatter.legend(title='Species')\nscatter.xaxis.set_label_text('Bill Length (mm)')\nscatter.yaxis.set_label_text('Body Mass (g)')\nscatter.plot()\n```\n\nQuarto makes it easy to include values retrieved from the data that will update if the data changes or we process it differently.\nThe average mass of Adelie penguins is `{python} float(round(grouped_data.loc[grouped_data['Species'] == 'Adelie', 'Body Mass (g)'].values[0], 1))`.\n\nWe can also use a different method:\n\n```{python}\n#| echo: false\n\naverage_mass_gentoo = grouped_data.loc[grouped_data['Species'] == 'Gentoo', 'Body Mass (g)'].values[0]\naverage_mass_gentoo = float(round(average_mass_gentoo, 1))\n```\n\nThe average mass of Gentoo penguins is `{python} average_mass_gentoo`.\n\nThis results in the following outputs:\nThe data contains measurements of penguin species, including bill length, bill depth, flipper length, and body mass. Table 6.1 provides summary statistics for these measurements by species.\n\n\n\n\nTable 6.1: Summary statistics by species\n\n\n\n\n\n\n\n\n\n\nSpecies\nBill Length (mm)\nBill Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n0\nAdelie\n38.8\n18.3\n190.0\n3700.7\n\n\n1\nChinstrap\n48.8\n18.4\n195.8\n3733.1\n\n\n2\nGentoo\n47.5\n15.0\n217.2\n5076.0\n\n\n\n\n\n\n\n\n\n\nWe can also visualize the relationship between bill length and body mass, as depicted in Figure 6.1.\n\n\n\n\n\n\n\n\nFigure 6.1: Scatter plot of bill length vs. body mass\n\n\n\n\n\nQuarto makes it easy to include values retrieved from the data that will update if the data changes or we process it differently. The average mass of Adelie penguins is 3700.7.\nWe can also use a different method:\nThe average mass of Gentoo penguins is 5076.0.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Replication with Python and Quarto: Best Practices</span>"
    ]
  },
  {
    "objectID": "python-replication.html#additional-considerations",
    "href": "python-replication.html#additional-considerations",
    "title": "Replication Example: Python + Quarto",
    "section": "6.4 Additional considerations",
    "text": "6.4 Additional considerations\nWith Quarto, one can split a document into multiple files and include them as needed. This is useful for organizing a research project, making it easier to manage and update individual components without affecting the entire document. For instance, you could have a file introduction.qmd for the introduction, methods.qmd for the methods section, and so on.\nSimilarly, you can write your main python (or R, Julia, etc.) code in separate script files and include them in your Quarto document using the include directive (it is common to use a _ prefix for documents that are not meant to be rendered). If you had a python file with:\n\n# _summarize.py\nimport pandas as pd\n\nworld_bank_data = pd.read_csv(\"https://example.com/world_bank_data.csv\")\n\n# Summarize the data\nsummary = world_bank_data.describe()\nsummary\n\nThen, you could include it in your Quarto document like this:\n---\ntitle: \"Data\"\n---\n\nThe summary statistics are as follows:\n\n{{&lt; include _summarize.py &gt;}}\nThis allows for better organization and modularity in your analysis workflow.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Replication with Python and Quarto: Best Practices</span>"
    ]
  },
  {
    "objectID": "git-collaboration.html",
    "href": "git-collaboration.html",
    "title": "7  Version Control with Git for Reproducible Research",
    "section": "",
    "text": "7.1 What is Git and Why Bother?\nGit is a version control system. Think of it as a “save” button for your entire project folder, not just a single file. It tracks every change you make to your code, allowing you to revert to any previous version at any time. For research, this is invaluable.\nWhy use Git?",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Version Control with Git for Reproducible Research</span>"
    ]
  },
  {
    "objectID": "git-collaboration.html#what-is-git-and-why-bother",
    "href": "git-collaboration.html#what-is-git-and-why-bother",
    "title": "7  Version Control with Git for Reproducible Research",
    "section": "",
    "text": "Reproducibility: You have a complete history of your analysis. A referee asks what you did six months ago? Git knows.\n\ngit log\ncommit 5f2652de4b9fb9f72e41996e78016bed6438a04e (HEAD -&gt; master)\nAuthor: Eric Roca &lt;eric.roca@gmail.com&gt;\nDate:   Fri Aug 29 19:36:30 2025 +0200\n\n    Commented\n\ncommit beccfd7db887e3ec894ae85088c5abcb6037e10d\nAuthor: Eric Roca &lt;eric.roca@gmail.com&gt;\nDate:   Fri Aug 19 9:19:44 2025 +0200\n\n    Initial analysis\n git diff beccfd7 5f2652d analysis.do\ndiff --git a/analysis.do b/analysis.do\nindex e2d2607..93f26b3 100644\n--- a/analysis.do\n+++ b/analysis.do\n@@ -1,5 +1,12 @@\n+/*\n+    Eric Roca Fernandez\n+*/\n+\n+// Open the database\n sysuse auto\n\n+// Summarize the turn data\n sum turn\n\n+// Initial regression\n reg weight mpg\n\nCollaboration: It makes working with co-authors seamless. No more emailing analysis_v3_final_Johns_edits.do.\nExperimentation: You can safely test new ideas (e.g., a different model specification) without breaking your main analysis.\nBackup: When used with a remote hosting service, it’s a backup of your entire codebase.\n\n\n7.1.1 Git vs. GitHub (and others)\nThis is a common point of confusion.\n\nGit is the software on your computer that tracks changes.\nGitHub, GitLab, Codeberg, and Forgejo are websites that host your Git projects. They are the “cloud” for your code, allowing you to store it remotely and share it with others.\n\nYou use Git locally on your machine, and then push your changes to a service like GitHub to collaborate or for backup.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Version Control with Git for Reproducible Research</span>"
    ]
  },
  {
    "objectID": "git-collaboration.html#the-basic-workflow",
    "href": "git-collaboration.html#the-basic-workflow",
    "title": "7  Version Control with Git for Reproducible Research",
    "section": "7.2 The Basic Workflow",
    "text": "7.2 The Basic Workflow\n\n7.2.1 1. Initializing a Repository\nIn your main project folder, run this command once:\ngit init\nThis creates a hidden .git folder where Git will store the entire history of your project.\n\n\n7.2.2 2. The .gitignore File: Tell Git What to Ignore\nThis is the most important step for a research project. You do not want to track data files, temporary files, or large output files (like PDFs or LaTeX tables) with Git. Git is for code.\nCreate a file named .gitignore in your project’s root directory and add the names of files and folders to ignore.\n# .gitignore\n\n# Data - NEVER track data with Git\ndata/\n/data/original/\n/data/temporary/\n/data/final/\n\n# Output files\npaper/\ntables/\nfigures/\noutput/\n\n# Stata specific\n*.dta.bak\n*.gph\n\n# Python specific\n__pycache__/\n*.pyc\n.ipynb_checkpoints\n\n# Quarto / R specific\n_freeze/\n_quarto/\n*.html\n*.pdf\n\n# OS-specific files\n.DS_Store\nThumbs.db\n\n\n7.2.3 3. Saving Your Work: Add and Commit\nSaving changes is a two-step process. You should only do these once you have made significant changes to your code, not every minor edit or every time you save. Think about Git as a history book: you would not write about every single detail, just the important events.\n\nStage changes with git add: Once you are ready to track changes, tell Git which files you want to include in your next snapshot.\n# Stage a specific file\ngit add code/analysis.do\n\n# Stage all changes in the current directory (respects .gitignore)\ngit add .\nCommit changes with git commit: Save the staged files as a snapshot in the project’s history. You must include a message describing the change.\ngit commit -m \"Add initial regression for model 1\"\n\nTip: Write clear, concise commit messages. They are your research logbook. “Fix bug” is bad. “Fix bug where sample was not correctly filtered for women” is good.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Version Control with Git for Reproducible Research</span>"
    ]
  },
  {
    "objectID": "git-collaboration.html#collaboration-push-pull-and-remotes",
    "href": "git-collaboration.html#collaboration-push-pull-and-remotes",
    "title": "7  Version Control with Git for Reproducible Research",
    "section": "7.3 Collaboration: Push, Pull, and Remotes",
    "text": "7.3 Collaboration: Push, Pull, and Remotes\nTo collaborate, you first need to host your repository on a service like GitHub. After creating a repository on the website, it will give you a URL. You link your local repository to it like this:\n# Add a remote named \"origin\" (a standard convention)\ngit remote add origin &lt;URL_from_GitHub&gt;\n\ngit push: Upload your committed changes to the remote repository.\ngit push origin main\ngit pull: Download changes from the remote repository. Always do this before you start working.\ngit pull origin main\n\n\n7.3.1 A Typical Co-author Workflow\n\nYou (start of day): git pull to get your co-author’s latest changes.\nYou: Work on your code, cleaning data or running a new analysis.\nYou: Save your work: git add . then git commit -m \"Add robustness check with alternative fixed effects\".\nYou (end of day): git push to upload your changes for your co-author to see.\nCo-author (next day): Repeats the cycle, starting with git pull.\n\n\n\n7.3.2 Merge Conflicts: When You Edit the Same Thing\nA merge conflict happens when you and a co-author change the same lines in the same file. Git doesn’t know which version to keep, so it asks you to decide.\nImagine you both edit line 5 of analysis.do. When you git pull, Git will stop and mark the file with a conflict:\n// ... some stata code ...\n\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nreg gdp growth controls, robust\n=======\nreg gdp growth controls, cluster(region)\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; fa345b2... Add clustered standard errors\n\n// ... more stata code ...\n\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD: This is your version.\n=======: Separates the two versions.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; fa345b2...: This is the incoming version from your co-author.\n\nTo resolve it:\n\nOpen the file in your editor.\nDelete the lines you don’t want to keep.\nDelete the Git markers (&lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, &gt;&gt;&gt;&gt;&gt;&gt;&gt;).\nThe final code should be exactly what you want it to be. For example:\n// ... some stata code ...\n\nreg gdp growth controls, cluster(region)\n\n// ... more stata code ...\nSave the file, then git add and git commit to finalize the merge.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Version Control with Git for Reproducible Research</span>"
    ]
  },
  {
    "objectID": "git-collaboration.html#branches-a-safe-place-to-experiment",
    "href": "git-collaboration.html#branches-a-safe-place-to-experiment",
    "title": "7  Version Control with Git for Reproducible Research",
    "section": "7.4 Branches: A Safe Place to Experiment",
    "text": "7.4 Branches: A Safe Place to Experiment\nBy default, you work on a branch called main (or master). A branch is like a parallel timeline for your project. They are incredibly useful for research.\n\n\n\n\n\ngitGraph:\n    commit id: \"Initial version\"\n    branch revision-1\n    checkout revision-1\n    commit id: \"Add new database\"\n    commit id: \"Regressions with new data\"\n    branch revision-1-use_probit\n    checkout revision-1-use_probit\n    commit id: \"Use probit model\"\n    checkout main\n    merge revision-1\n\n\n\n\n\n\nWhy use branches?\n\nExperiment safely: Want to try a completely new statistical method? Create a branch. If it doesn’t work out, you can just delete the branch, and your main code is untouched.\nRespond to referee reports: This is a killer feature. When you get a “revise and resubmit,” create a branch for it.\n# Create a new branch for the first revision and switch to it\ngit checkout -b revision-1\nNow you can make all the changes the referees requested. Your main branch still represents the original submitted version. When you’re done, you can merge the changes from revision-1 back into main.\n\n\n7.4.1 Branching Workflow\n\nCreate a new branch: bash     git branch try-new-instrument\nSwitch to it: bash     git checkout try-new-instrument\nWork as usual: add, commit, push. Your changes are isolated on this branch.\nMerge it back (if successful): When you’re happy with the experiment, switch back to main and merge the changes.\ngit checkout main\ngit merge try-new-instrument",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Version Control with Git for Reproducible Research</span>"
    ]
  }
]